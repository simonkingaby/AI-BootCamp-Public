{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chest X-Ray Images (Pneumonia)\n",
    "\n",
    "Go here [Download Kaggle Datafile](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)\n",
    "\n",
    "## Part 1: Consolidating the File Folders\n",
    "\n",
    "Then, scroll down and change the paths at the bottom.\n",
    "Then, run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the images in the folders and subfolders, \n",
    "# rename the files after the folder's names, \n",
    "# then move all the files to a new folder\n",
    "\n",
    "def locate_images(start_path, new_folder):\n",
    "    # set the current working directory to the start path\n",
    "    os.chdir(start_path)\n",
    "    # get the current working directory\n",
    "    cwd = os.getcwd()\n",
    "    # list all the files and folders in the current working directory\n",
    "    files = os.listdir(cwd)\n",
    "    # loop through the files to locate the image files\n",
    "    for file in files:\n",
    "        # check if the file is a directory\n",
    "        if os.path.isdir(file):\n",
    "             # change the current working directory to the directory\n",
    "            os.chdir(file)\n",
    "            # recurse\n",
    "            start_path = os.getcwd()\n",
    "            locate_images(start_path, new_folder)\n",
    "            os.chdir('..')\n",
    "        # check if the file is an image file\n",
    "        if file.endswith('.jpg') or file.endswith('.png') or file.endswith('.jpeg'):\n",
    "            move_image(file, new_folder)\n",
    "\n",
    "def move_image(file, new_folder):\n",
    "    print(f\"Moving... {file}\")\n",
    "    # get the current working directory\n",
    "    folder_name = os.getcwd()\n",
    "    # get the new name of the file\n",
    "    new_name = os.path.basename(folder_name) + '_' + file\n",
    "    # rename the file to move it from the current working directory to the new folder\n",
    "    if not os.path.exists(new_folder + new_name):\n",
    "        os.rename(file, new_folder + new_name)\n",
    "\n",
    "def main():\n",
    "    # Change these paths to match your drive\n",
    "    start_path = \"F:/Downloads/lungs/chest_xray/\"\n",
    "    new_folder = \"F:/Downloads/lungs/All/\"\n",
    "    locate_images(start_path, new_folder)\n",
    "    \n",
    "# run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_folder = \"F:/Downloads/lungs/All/\"\n",
    "images = []\n",
    "list_of_images = os.listdir(new_folder)\n",
    "for image in list_of_images:\n",
    "    images.append(Image.open(new_folder + image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the images to grayscale, most are already, but some are full-color\n",
    "grayscale_images = []\n",
    "for image in images:\n",
    "    grayscale_images.append(image.convert('L'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a for loop to resize all images\n",
    "target_size = (250, 250)\n",
    "resized_images = [img.resize(target_size, resample = Image.LANCZOS) for img in grayscale_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(250, 250)}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the resizing of all images\n",
    "# Get all the sizes into a list, then convert to a set\n",
    "sizes = set([img.size for img in resized_images])\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Values:\n",
      "[[29. 33. 32. ... 34. 31. 29.]\n",
      " [30. 33. 31. ... 31. 33. 30.]\n",
      " [29. 32. 32. ... 31. 33. 29.]\n",
      " ...\n",
      " [27. 30. 29. ... 33. 34. 30.]\n",
      " [26. 30. 29. ... 33. 35. 31.]\n",
      " [26. 31. 29. ... 32. 33. 31.]]\n"
     ]
    }
   ],
   "source": [
    "# Convert all images to floating point numpy arrays\n",
    "float_images = [np.array(img).astype(np.float32) for img in resized_images]\n",
    "\n",
    "# Display the pixel values of the first image\n",
    "print(\"Pixel Values:\")\n",
    "print(float_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Values:\n",
      "[[0.11372549 0.12941177 0.1254902  ... 0.13333334 0.12156863 0.11372549]\n",
      " [0.11764706 0.12941177 0.12156863 ... 0.12156863 0.12941177 0.11764706]\n",
      " [0.11372549 0.1254902  0.1254902  ... 0.12156863 0.12941177 0.11372549]\n",
      " ...\n",
      " [0.10588235 0.11764706 0.11372549 ... 0.12941177 0.13333334 0.11764706]\n",
      " [0.10196079 0.11764706 0.11372549 ... 0.12941177 0.13725491 0.12156863]\n",
      " [0.10196079 0.12156863 0.11372549 ... 0.1254902  0.12941177 0.12156863]]\n"
     ]
    }
   ],
   "source": [
    "# To normalize images to a range between 0 and 1,\n",
    "# we need to divide all pixel values by the max of 255\n",
    "\n",
    "normalized_images = [img/255 for img in float_images]\n",
    "\n",
    "# Display the pixel values of the first image\n",
    "print(\"Pixel Values:\")\n",
    "print(normalized_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the list of images to a numpy array where 0 = Normal and 1 = Pneumonia\n",
    "def feature_value(file):\n",
    "    if file.startswith('NORMAL'):\n",
    "        return 0 # Normal\n",
    "    else:\n",
    "        return 1 # Pneumonia\n",
    "\n",
    "# Put the Normal and Pneumonia labels into a numpy array as y\n",
    "y = np.array([feature_value(file) for file in list_of_images]).reshape(-1,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of images to a numpy array as X\n",
    "X = normalized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll split our data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21960\n",
      "21960\n"
     ]
    }
   ],
   "source": [
    "# Apply augmentation to the whole training dataset\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,      # Random rotation (degrees)\n",
    "    width_shift_range=0.1,  # Random horizontal shift\n",
    "    height_shift_range=0.1, # Random vertical shift\n",
    "    shear_range=0.2,        # Shear intensity\n",
    "    zoom_range=0.2,         # Random zoom\n",
    "    horizontal_flip=True,   # Random horizontal flip\n",
    "    vertical_flip=False,    # No vertical flip for face images\n",
    "    fill_mode='nearest'     # Fill mode for handling newly created pixels\n",
    ")\n",
    "\n",
    "# Create variables to hold the X and y training data\n",
    "X_train_aug = []\n",
    "y_train_aug = []\n",
    "\n",
    "# Loop through all the images.\n",
    "for i in range(len(X_train)):\n",
    "    # Select the image\n",
    "    img = X_train[i]\n",
    "    # Select the label from the training data\n",
    "    label = y_train[i]\n",
    "    \n",
    "    # Add a channel dimension for grayscale images\n",
    "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
    "\n",
    "    # Ensure that the input data has the correct shape\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Add 5 images for every original image\n",
    "    for j in range(5):\n",
    "        # Append a new image to the X list\n",
    "        X_train_aug.append(datagen.flow(img, batch_size=1).next()[0])\n",
    "        # Append the label for the original image to the y list\n",
    "        y_train_aug.append(label)\n",
    "\n",
    "# Print the length of each list\n",
    "print(len(X_train_aug))\n",
    "print(len(y_train_aug))\n",
    "\n",
    "# took 10 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 250, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape test data for the model\n",
    "X_test_np = []\n",
    "for img in X_test:\n",
    "    # Add a channel dimension for grayscale images\n",
    "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
    "    # Append the image to the list\n",
    "    X_test_np.append(img)\n",
    "\n",
    "# Convert to numpy array\n",
    "X_test_np = np.array(X_test_np)\n",
    "\n",
    "# Check the shape of the first image\n",
    "X_test_np[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state of our data to pickle files so we don't have to do all this again\n",
    "import pickle\n",
    "\n",
    "# Save the training data\n",
    "with open('X_train_aug.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_aug, f)\n",
    "with open('y_train_aug.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_aug, f)\n",
    "\n",
    "# Save the testing data\n",
    "with open('X_test_np.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test_np, f)\n",
    "with open('y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "\n",
    "# Took 8 minutes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values to numpy arrays\n",
    "X_train_aug_np = np.array(X_train_aug)\n",
    "X_test_np = np.array(X_test_np)\n",
    "y_train_aug_np = np.array(y_train_aug)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_aug_np, y_train_aug_np, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17568, 250, 250, 1)\n",
      "(4392, 250, 250, 1)\n",
      "(1464, 250, 250, 1)\n",
      "(17568, 1)\n",
      "(4392, 1)\n",
      "(1464, 1)\n"
     ]
    }
   ],
   "source": [
    "# print shapes of the training, validation, and test sets\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test_np.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(250, 250, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # 2 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "549/549 [==============================] - 517s 936ms/step - loss: 0.3253 - accuracy: 0.8585 - val_loss: 0.2347 - val_accuracy: 0.9016\n",
      "Epoch 2/10\n",
      "549/549 [==============================] - 497s 906ms/step - loss: 0.2184 - accuracy: 0.9138 - val_loss: 0.2154 - val_accuracy: 0.9126\n",
      "Epoch 3/10\n",
      "549/549 [==============================] - 497s 905ms/step - loss: 0.1797 - accuracy: 0.9305 - val_loss: 0.1830 - val_accuracy: 0.9283\n",
      "Epoch 4/10\n",
      "549/549 [==============================] - 496s 904ms/step - loss: 0.1291 - accuracy: 0.9500 - val_loss: 0.2028 - val_accuracy: 0.9335\n",
      "Epoch 5/10\n",
      "549/549 [==============================] - 497s 906ms/step - loss: 0.0887 - accuracy: 0.9662 - val_loss: 0.2177 - val_accuracy: 0.9196\n",
      "Epoch 6/10\n",
      "549/549 [==============================] - 497s 905ms/step - loss: 0.0515 - accuracy: 0.9810 - val_loss: 0.2532 - val_accuracy: 0.9287\n",
      "Epoch 7/10\n",
      "549/549 [==============================] - 497s 905ms/step - loss: 0.0260 - accuracy: 0.9906 - val_loss: 0.3302 - val_accuracy: 0.9194\n",
      "Epoch 8/10\n",
      "549/549 [==============================] - 498s 906ms/step - loss: 0.0245 - accuracy: 0.9908 - val_loss: 0.3684 - val_accuracy: 0.9192\n",
      "Epoch 9/10\n",
      "549/549 [==============================] - 498s 908ms/step - loss: 0.0153 - accuracy: 0.9944 - val_loss: 0.3815 - val_accuracy: 0.9285\n",
      "Epoch 10/10\n",
      "549/549 [==============================] - 497s 906ms/step - loss: 0.0110 - accuracy: 0.9957 - val_loss: 0.4595 - val_accuracy: 0.9285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x260898c8810>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "def train(model, X_train, y_train, X_val, y_val, epochs):\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs\n",
    "    )\n",
    "    return history\n",
    "\n",
    "train(model, X_train, y_train, X_val, y_val, epochs=10)\n",
    "\n",
    "# NOTE: This takes a while to run ~ 10 minutes per epoch. 90 minutes in all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 11s 246ms/step - loss: 0.4693 - accuracy: 0.9201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4692945182323456, 0.9200819730758667]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, evaluate the model with the test data we originally reserved with the first train_test_split\n",
    "model.evaluate(X_test_np, y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
